{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13110248,"sourceType":"datasetVersion","datasetId":8304775},{"sourceId":13507668,"sourceType":"datasetVersion","datasetId":8484917},{"sourceId":13595013,"sourceType":"datasetVersion","datasetId":8477943},{"sourceId":13635409,"sourceType":"datasetVersion","datasetId":8616436},{"sourceId":267833412,"sourceType":"kernelVersion"},{"sourceId":267833596,"sourceType":"kernelVersion"},{"sourceId":267833616,"sourceType":"kernelVersion"},{"sourceId":271362351,"sourceType":"kernelVersion"},{"sourceId":273129712,"sourceType":"kernelVersion"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"> # ðŸŒˆ Step 1: Data Processing","metadata":{}},{"cell_type":"markdown","source":"* * * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4>âœ¨ Read accelerometer and gyroscope data</br></br>\nâœ¨ Create required dataframe columns such as participant, label and category</br></br>\nâœ¨ Merge the accelerometer data and gyroscope data</br></br>\nâœ¨ Resample the merged data</br></br>\nâœ¨ Export the preprocessed data</h4>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom glob import glob\nfrom pathlib import Path\n\n\nsensor_data_files = glob(\"/kaggle/input/sensor/*.csv\")\n\ndef merge_sensor_file_data(sensor_data_files):\n    \n    common_path = '/kaggle/input/sensor/'\n    \n    # create empty data frames each for accelerometer and gyroscope\n    df_accelerometer = pd.DataFrame()\n    df_gyroscope = pd.DataFrame()\n\n    set_accelerometer = 0\n    set_gyroscope = 0\n\n\n    # create a for loop that appends the CSV files into the two data frames, each for accelerometer and gyroscope\n    for file in sensor_data_files :\n        participant = file.split(\"-\")[0].replace(common_path,\"\")\n        label = file.split(\"-\")[1]\n        category = file.split(\"-\")[2].rstrip(\"123\").rstrip(\"_MetaWear_2019\")\n    \n        df = pd.read_csv(file)\n        df[\"participant\"] = participant\n        df[\"label\"] = label\n        df[\"category\"] = category\n\n\n        if 'Accelerometer' in file :\n            set_accelerometer += 1\n            df[\"set\"] = set_accelerometer\n            df_accelerometer = pd.concat([df_accelerometer,df])\n     \n\n        if 'Gyroscope' in file :\n            set_gyroscope += 1\n            df[\"set\"] = set_gyroscope\n            df_gyroscope = pd.concat([df_gyroscope,df])\n       \n        \n    #let's use epoch (unix time) as the standard time as it will not reflect the summer time change in our project\n   \n    df_accelerometer.index = pd.to_datetime(df_accelerometer[\"epoch (ms)\"],unit=\"ms\")\n    df_gyroscope.index = pd.to_datetime(df_gyroscope[\"epoch (ms)\"],unit=\"ms\")\n\n\n    \n    columns = [\"epoch (ms)\",\"time (01:00)\", \"elapsed (s)\"]\n    df_accelerometer.drop(columns,axis=1,inplace=True)\n    df_gyroscope.drop(columns,axis=1,inplace=True)\n\n    df_merged = pd.concat([df_accelerometer.iloc[:,0:3],df_gyroscope], axis=1)\n\n    df_merged.columns = [\n    \"acc_x\",\n    \"acc_y\",\n    \"acc_z\",\n    \"gyr_x\",\n    \"gyr_y\",\n    \"gyr_z\",\n    \"participant\",\n    \"label\",\n    \"category\",\n    \"set\"\n    ]\n    \n    return df_merged\n\n\ndef Sample(df_merged):\n    # Sample Data\n    sampling = {\n        \"acc_x\": \"mean\",\n        \"acc_y\": \"mean\",\n        \"acc_z\": \"mean\",\n        \"gyr_x\": \"mean\",\n        \"gyr_y\": \"mean\",\n        \"gyr_z\": \"mean\",\n        \"participant\": \"last\",\n        \"label\": \"last\",\n        \"category\": \"last\",\n        \"set\": \"last\"\n    }\n\n    #let us split the dataframes based on the date which uses less computational power\n    days = [g for n, g in df_merged.groupby(pd.Grouper(freq='D'))]\n    data_resampled = pd.concat([df.resample(rule='200ms').apply(sampling).dropna() for df in days])\n    data_resampled[\"set\"] = data_resampled[\"set\"].astype(\"int\")\n\n    return data_resampled\n\n\n# read and merge sensory data files\ndf_merged = merge_sensor_file_data(sensor_data_files)\n\n#sample the data\ndf_resampled = Sample(df_merged)\n\n\n# Export dataset\nPath('/kaggle/working/interim1').mkdir(parents=True, exist_ok=True)\ndf_resampled.to_pickle(\"/kaggle/working/interim1/01_data_processed.pkl\")\nprint(df_resampled[\"label\"].unique())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:13:24.373534Z","iopub.execute_input":"2025-11-06T22:13:24.373893Z","iopub.status.idle":"2025-11-06T22:13:25.527855Z","shell.execute_reply.started":"2025-11-06T22:13:24.373872Z","shell.execute_reply":"2025-11-06T22:13:25.526742Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"> # ðŸŒˆ Step 2 : Visualize","metadata":{}},{"cell_type":"markdown","source":" <h4> âœ¨ Plot particular sensor column (gyr_x) for each label  (bench,Ohp,squat,dead,row,rest) </h4>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom IPython.display import display\nfrom pathlib import Path\n\n\nmpl.style.use(\"seaborn-v0_8-deep\")\nmpl.rcParams[\"figure.figsize\"] = (15,5)\nmpl.rcParams[\"figure.dpi\"] = 100\n\n\ndf_resampled = pd.read_pickle(\"/kaggle/input/interim1/01_data_processed.pkl\")\n\n\n# Select few data sets and plot for each label for gyr_x\ncol = \"gyr_x\"\nfor label in df_resampled[\"label\"].unique():\n    subset = df_resampled[df_resampled[\"label\"] == label]\n    fig, ax = plt.subplots()\n    plt.plot(subset[:50][col].reset_index(drop = True), label=label,color=\"orange\")\n    plt.legend()\n    plt.show() \n    #Path('/kaggle/working/visualization-images/folder1').mkdir(parents=True, exist_ok=True)\n    #plt.savefig(f\"/kaggle/working/visualization-images/folder1/{label.title()} ({col}).png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T23:31:30.998622Z","iopub.execute_input":"2025-11-06T23:31:30.998889Z","iopub.status.idle":"2025-11-06T23:31:31.763610Z","shell.execute_reply.started":"2025-11-06T23:31:30.998871Z","shell.execute_reply":"2025-11-06T23:31:31.762647Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Compare each participant based on a specific label(ohp) and a particular sensor column(acc_z) </h4>","metadata":{}},{"cell_type":"code","source":"# Compare participants\n\nfrom pathlib import Path\n\nsubset = []\ncompare_column = \"participant\"\ncol = \"acc_z\"\nlabel = \"ohp\"\nsubset = df_resampled.query(\"label == @label\").sort_values(compare_column).reset_index()\nsubset.groupby([compare_column])[col].plot()\n#ax.set_xlabel(\"acc_z\")\n#ax.set_ylabel(\"acc_z reading\")\nplt.legend() \nplt.xlabel(\"Samples\")\nplt.ylabel(f\"{col} reading\")\n\n#Path('/kaggle/working/visualization-images/Compare-participants').mkdir(parents=True, exist_ok=True)\n#plt.savefig(f\"/kaggle/working/visualization-images/Compare-participants/{label.title()} ({col}).png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" ** * * * * * **   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Compare category based on a specific label(ohp),  particular sensor column(gyr_z) and  participant(A) </h4> ","metadata":{}},{"cell_type":"code","source":"# Compare category - Heavy vs Medium\n\nfrom pathlib import Path\n\nsubset = []\nparticipant1 = \"A\"\ncol = \"gyr_z\"\nlabel = \"ohp\"\ncompare_column = \"category\"\nsubset = df_resampled.sort_values(by=['participant']).query(\"label == @label\").query(\"participant == @participant1\").reset_index()[300:600]\nfig, ax = plt.subplots()\nsubset.groupby([compare_column])[col].plot()\n#set x axis and y axis values in pandas plot\nax.set_xlabel(\"Samples\")\nax.set_ylabel(f\"{col} reading\")\nplt.title(f\"Participant {participant1}\")\nplt.legend() \n\n#Path('/kaggle/working/visualization-images/Compare_category(Heavy vs Medium)').mkdir(parents=True, exist_ok=True)\n#plt.savefig(f\"/kaggle/working/visualization-images/Compare_category(Heavy vs Medium)/{label.title()} ({col}).png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:38:18.855379Z","iopub.execute_input":"2025-11-06T22:38:18.855847Z","iopub.status.idle":"2025-11-06T22:38:19.175715Z","shell.execute_reply.started":"2025-11-06T22:38:18.855710Z","shell.execute_reply":"2025-11-06T22:38:19.174798Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Compare gyroscope data (gyr_x, gyr_y, gyr_z) for a particular label(squat) and  participant(A) </h4>","metadata":{}},{"cell_type":"code","source":"\nlabel = \"squat\"\nparticipant = \"A\"\nsubset = df_resampled.query(f\"label == '{label}'\").query(f\"participant == '{participant}'\").reset_index()[:50]\nsubset[[\"gyr_x\", \"gyr_y\", \"gyr_z\"]].plot()\nax.set_xlabel(\"values\")\nax.set_ylabel(\"samples\")\nplt.title(f\"{label} {participant}\")\nplt.legend() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:36:45.603041Z","iopub.execute_input":"2025-11-06T22:36:45.603332Z","iopub.status.idle":"2025-11-06T22:36:45.807088Z","shell.execute_reply.started":"2025-11-06T22:36:45.603316Z","shell.execute_reply":"2025-11-06T22:36:45.806395Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4>  âœ¨ Plot all sensor data for every label and participant </h4>","metadata":{}},{"cell_type":"code","source":"# Create a loop to plot all combinations per sensor\n\nfrom pathlib import Path\n\n\nlabels = df_resampled[\"label\"].unique()\nparticipants = df_resampled[\"participant\"].unique()\n\nfor label in labels:\n    for participant in participants:\n        subset = df_resampled.query(f\"label == '{label}'\").query(f\"participant == '{participant}'\").reset_index()\n    \n\n        if len(subset)>0 :\n            fig, ax = plt.subplots()\n            subset[[\"gyr_x\", \"gyr_y\", \"gyr_z\"]].plot(ax=ax)\n            ax.set_xlabel(\"gyroscope samples\")\n            ax.set_ylabel(\"gyroscope reading\")\n            plt.title(f\"{label} {participant}\".title())\n            plt.legend()\n            #Path('/kaggle/working/visualization-images/All_Labels_&_Participant/Gyroscope').mkdir(parents=True, exist_ok=True)\n            #plt.savefig(f\"/kaggle/working/visualization-images/All_Labels_&_Participant/Gyroscope/{label.title()} ({participant}).png\")\n\n\nlabels = df_resampled[\"label\"].unique()\nparticipants = df_resampled[\"participant\"].unique()\n\nfor label in labels:\n    for participant in participants:\n        subset = df_resampled.query(f\"label == '{label}'\").query(f\"participant == '{participant}'\").reset_index()\n    \n\n        if len(subset)>0 :\n            fig, ax = plt.subplots()\n            subset[[\"acc_x\", \"acc_y\", \"acc_z\"]].plot(ax=ax)\n            ax.set_xlabel(\"accelerometer samples\")\n            ax.set_ylabel(\"accelerometer reading\")\n            plt.title(f\"{label} {participant}\".title())\n            plt.legend() \n            #Path('/kaggle/working/visualization-images/All_Labels_&_Participant/Accelerometer').mkdir(parents=True, exist_ok=True)\n            #plt.savefig(f\"/kaggle/working/visualization-images/All_Labels_&_Participant/Accelerometer/{label.title()} ({participant}).png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * * * * ***   ","metadata":{}},{"cell_type":"code","source":"# Combine plots in one figure\nlabel = \"row\"\nparticipant = \"D\"\nsubset = df_resampled.query(f\"label == '{label}'\").query(f\"participant == '{participant}'\").reset_index()\n\nfig , ax = plt.subplots(nrows=2, sharex=True, figsize=(20,10))\nsubset[[\"acc_x\", \"acc_y\", \"acc_z\"]].plot(ax=ax[0])\nsubset[[\"gyr_x\", \"gyr_y\", \"gyr_z\"]].plot(ax=ax[1])\nplt.legend()\n\n#just have some styling\nax[0].set_xlabel(\"acc samples\") \nax[0].set_ylabel(\"acc reading\") \nax[1].set_xlabel(\"gyr samples\")\nax[1].set_ylabel(\"gyr reading\") \nax[0].legend(loc=\"upper left\",bbox_to_anchor=(0.1,1.15), ncol=3, fancybox=True, shadow=True)\nax[1].legend(loc=\"upper left\",bbox_to_anchor=(0.1,1.15), ncol=3, fancybox=True, shadow=True) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Plot all sensor data for every label and participant </h4>","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n      \n# Loop over all combinations and export for both sensors\nlabels = df_resampled[\"label\"].unique()\nparticipants = df_resampled[\"participant\"].unique()\n\nfor label in labels:\n    for participant in participants:\n        subset = df_resampled.query(f\"label == '{label}'\").query(f\"participant == '{participant}'\").reset_index()\n    \n\n        if len(subset)>0 :\n            fig , ax = plt.subplots(nrows=2, sharex=True, figsize=(20,10))\n            subset[[\"acc_x\", \"acc_y\", \"acc_z\"]].plot(ax=ax[0])\n            subset[[\"gyr_x\", \"gyr_y\", \"gyr_z\"]].plot(ax=ax[1])\n\n            ax[0].legend(loc=\"upper center\",bbox_to_anchor=(0.1,1.15), ncol=3, fancybox=True, shadow=True)\n            ax[1].legend(loc=\"upper center\",bbox_to_anchor=(0.1,1.15), ncol=3, fancybox=True, shadow=True)\n            ax[1].set_xlabel(\"samples\")\n            ax[0].set_ylabel(\"accelerometer readings\")\n            ax[1].set_ylabel(\"gyroscope readings\")\n            ax[0].set_title(f\"Participant : {participant}\\nExercise : {label}\")\n            #Path('/kaggle/working/visualization-images/All_Labels_&_Participant/Accelerometer_Gyroscope').mkdir(parents=True, exist_ok=True)\n            #plt.savefig(f\"/kaggle/working/visualization-images/All_Labels_&_Participant/Accelerometer_Gyroscope/{label.title()} ({participant}).png\")\n\n# You can get the visualization images under (Datasets -> Visualization-images)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"> # ðŸŒˆ Step 3: Outlier detection ","metadata":{}},{"cell_type":"markdown","source":"<h3> âœ¨ Defining the three different methods for outlier detection </h3>\n\n<h4> 1.Interquartile range</h4>\n<h4> 2.Chauvenet's criterion </h4>\n<h4> 3.Local Outlier Factor </h4>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport scipy\nfrom sklearn.neighbors import LocalOutlierFactor  # pip install scikit-learn\nfrom pathlib import Path\n\nplt.style.use(\"fivethirtyeight\")\nplt.rcParams[\"figure.figsize\"] = (20,5)\nplt.rcParams[\"figure.dpi\"] = 100\n\n\n# Method 1 : Interquartile range (distribution-based)\ndef iqr(df, column):\n\n    df = df.copy()\n\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n\n    df[column + \"_outlier\"] = (df[column] < lower_bound) | (\n        df[column] > upper_bound\n    )\n\n    return df\n\n\n# Method 2  : Insert Chauvenet's function\ndef chauvenet(df, col, C=2):\n    \n    df = df.copy()\n    # Compute the mean and standard deviation.\n    mean = df[col].mean()\n    std = df[col].std()\n    N = len(df.index)\n    criterion = 1.0 / (C * N)\n\n    # Consider the deviation for the data points.\n    deviation = abs(df[col] - mean) / std\n\n    # Express the upper and lower bounds.\n    low = -deviation / math.sqrt(C)\n    high = deviation / math.sqrt(C)\n    prob = []\n    mask = []\n\n    # Pass all rows in the dataset.\n    for i in range(0, len(df.index)):\n        # Determine the probability of observing the point\n        prob.append(\n            1.0 - 0.5 * (scipy.special.erf(high[i]) - scipy.special.erf(low[i]))\n        )\n        # And mark as an outlier when the probability is below our criterion.\n        mask.append(prob[i] < criterion)\n    df[col + \"_outlier\"] = mask\n    return df\n\n\n\n\n# Method 3 : Local outlier factor (distance based)\ndef local_outlier_factor(df, columns, n=20):\n    \n    df = df.copy()\n\n    lof = LocalOutlierFactor(n_neighbors=n)\n    data = df[columns]\n    outliers = lof.fit_predict(data)\n    scores = lof.negative_outlier_factor_\n\n    df[\"outlier_lof\"] = outliers == -1\n    return df, outliers, scores\n\n\n\n# Plotting outliers\ndef visualize_outliers(df, col, outlier_col,method, save_plot):\n    \n    df = df.dropna(axis=0, subset=[col, outlier_col])\n    df[outlier_col] = df[outlier_col].astype(\"bool\")\n\n    fig, ax = plt.subplots()\n\n    plt.xlabel(\"samples\")\n    plt.ylabel(col + \"  reading\")\n\n   # Plot non outliers in default color\n    ax.plot(\n        df.index[~df[outlier_col]],\n        df[col][~df[outlier_col]],\n        \"+\",\n    )\n    # Plot data points that are outliers in red\n    ax.plot(\n        df.index[df[outlier_col]],\n        df[col][df[outlier_col]],\n        \"r+\",\n    )\n\n    plt.legend(\n        [ \"no outlier \" + col, \"outlier \" + col],\n        loc=\"upper center\",\n        ncol=2,\n        fancybox=True,\n        shadow=True,\n    )\n    #plt.show()\n\n    if (save_plot) :\n        Path(f'/kaggle/working/Outlier_detection/{method}').mkdir(parents=True, exist_ok=True)\n        plt.savefig(f\"/kaggle/working/Outlier_detection/{method}/{col}.png\")\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:41:18.785528Z","iopub.execute_input":"2025-11-06T22:41:18.785847Z","iopub.status.idle":"2025-11-06T22:41:19.009357Z","shell.execute_reply.started":"2025-11-06T22:41:18.785822Z","shell.execute_reply":"2025-11-06T22:41:19.008283Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Utilizing InterQuartile range </h4>","metadata":{}},{"cell_type":"code","source":"\n# Apply every method to detect outliers\n# Apply Method 1 : Interquartile range \n\n\n\nmerged_df = pd.read_pickle(\"/kaggle/input/interim1/01_data_processed.pkl\")\ncolumns = list(merged_df.columns[:6])\n\nsave_plot = True # Assign true if you want to save plot \n\nfor c in columns:\n    df_with_outlier_column  = iqr(merged_df, c)\n    df_with_outlier_column.reset_index()\n    visualize_outliers(df = df_with_outlier_column, col=c, outlier_col = c +\"_outlier\",method=\"IQR\",save_plot=save_plot)  \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:15:35.760596Z","iopub.execute_input":"2025-11-06T20:15:35.760861Z","iopub.status.idle":"2025-11-06T20:15:37.645900Z","shell.execute_reply.started":"2025-11-06T20:15:35.760843Z","shell.execute_reply":"2025-11-06T20:15:37.644960Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Utilizing Chauvenet's criterion </h4>","metadata":{}},{"cell_type":"code","source":"# Apply Method 2 : Insert Chauvenet's criterion\n\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsave_plot = True # Assign true if you want to save plot \nmerged_df = pd.read_pickle(\"/kaggle/input/interim1/01_data_processed.pkl\")\ncolumns = list(merged_df.columns[:6])\nfor c in columns:\n    df_with_outlier_column = chauvenet(merged_df, c)\n    df_with_outlier_column.reset_index()\n    visualize_outliers(df = df_with_outlier_column, col=c, outlier_col = c +\"_outlier\",method=\"Chauvenet\",save_plot=save_plot)   \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:43:26.748397Z","iopub.execute_input":"2025-11-06T22:43:26.748736Z","iopub.status.idle":"2025-11-06T22:43:29.627360Z","shell.execute_reply.started":"2025-11-06T22:43:26.748716Z","shell.execute_reply":"2025-11-06T22:43:29.626500Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Utilizing Local outlier factor </h4>","metadata":{}},{"cell_type":"code","source":"# Apply Method 3 : Local outlier factor (distance based)\n\nfrom pathlib import Path\nmerged_df = pd.read_pickle(\"/kaggle/input/interim1/01_data_processed.pkl\")\ncolumns = list(merged_df.columns[:6])\nsave_plot = True # Assign true if you want to save plot \n\ndf_with_outlier_column, outliers, scores = local_outlier_factor(df = merged_df, columns = columns)\n\nfor c in columns: \n    visualize_outliers(df = df_with_outlier_column , col = c , outlier_col = \"outlier_lof\",method=\"Local_outlier_factor\",save_plot=save_plot) \n ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Among the three approaches, selecting Chauvenet's criterion </h4>\n\n <font size=\"3\"> Replace outliers with NaN and export the dataset </font>","metadata":{}},{"cell_type":"code","source":"# Choose the best method : Among the three methods such as IQR, chauvenet's criteria and local outlier factor: Chauvenet's criteria identifies few outliers\nimport warnings\nwarnings.filterwarnings('ignore')\n\noutliers_removed_df = merged_df.copy()\nfor col in columns:\n    for label in merged_df[\"label\"].unique():\n        df_with_outlier_column = chauvenet(merged_df[merged_df[\"label\"]==label], col)\n        df_with_outlier_column.loc[df_with_outlier_column[col + \"_outlier\"],col] = np.nan\n\n        outliers_removed_df.loc[(outliers_removed_df[\"label\"]==label),col] = df_with_outlier_column[col]\n            \n\noutliers_removed_df.to_pickle(\"/kaggle/working/interim1/02_outliers_removed_chauvenets.pkl\") \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"> #  **ðŸŒˆ _Step 4: Feature Engineering_**","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">\nðŸ“š <font color=\"black\"> 4.1 : Handling outliers - <font color=\"royalblue\"> <span style=\"font-weight:bold\"> Interpolation </span> </br>\nðŸ“š <font color=\"black\"> 4.2 : Reduce noise/smoothing data - <font color=\"royalblue\"> <span style=\"font-weight:bold\"> Kalman filter <span style=\"font-weight:normal\">vs </span> Butter worth low pass filter </span> </br>\nðŸ“š <font color=\"black\"> 4.3 : Reduce dimensionality of datasets  - <font color=\"royalblue\"> <span style=\"font-weight:bold\"> Principal component Analysis </span> </br>\nðŸ“š  <font color=\"black\"> 4.4 : Qunatify model variability/Model optimization - <font color=\"royalblue\"> <span style=\"font-weight:bold\"> Sum of sqaures attribute </span> </br>\nðŸ“š <font color=\"black\">  4.5 : Temporal Abstraction - <font color=\"royalblue\"> <span style=\"font-weight:bold\"> Rolling mean </span> </br>\nðŸ“š <font color=\"black\">  4.6 : Frequency Abstraction  - <font color=\"royalblue\"> <span style=\"font-weight:bold\"> Variational Mode Decomposition <span style=\"font-weight:normal\">vs </span> Fourier transformation </span> </br>\nðŸ“š  <font color=\"black\"> 4.7 : Resolve overlapping windows\n</font>","metadata":{}},{"cell_type":"code","source":"!pip install vmdpy \n!pip install pykalman\nimport pywt\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom IPython.display import display\nfrom datatransformation1 import LowPassFilter, PrincipalComponentAnalysis\nfrom frequencyabstractionvmd import Variational_Mode_Decomposition\nfrom frequencyabstraction1 import FourierTransformation\nfrom temporalabstraction1 import NumericalAbstraction\nfrom sklearn.cluster import KMeans\nfrom multiprocessing import Pool\nfrom scipy.signal import savgol_filter\nfrom pykalman import KalmanFilter\nfrom scipy.signal import hilbert\nfrom vmdpy import VMD  #  pip install vmdpy\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"vmdpy\")\n\n\nplt.style.use(\"fivethirtyeight\")\nplt.rcParams[\"figure.figsize\"] = (20,5)\nplt.rcParams[\"figure.dpi\"] = 100\nplt.rcParams[\"lines.linewidth\"] = 2\n\n\ndf = pd.read_pickle(\"/kaggle/input/interim1/02_outliers_removed_chauvenets.pkl\")\ndisplay(df)\npredictor_columns = list(df.columns[:6])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:47:52.141853Z","iopub.execute_input":"2025-11-06T22:47:52.142123Z","iopub.status.idle":"2025-11-06T22:47:57.899790Z","shell.execute_reply.started":"2025-11-06T22:47:52.142105Z","shell.execute_reply":"2025-11-06T22:47:57.898954Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h3> ðŸ“š 4.1 Dealing outliers with interpolation(imputation)</h3>","metadata":{}},{"cell_type":"code","source":"# Dealing with missing values\nfor column in predictor_columns:\n    df[column] = df[column].interpolate()\n\nprint(\"After dealing missing values with interpolation in each column  \\n\")\nprint(df.isnull().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:47:59.188208Z","iopub.execute_input":"2025-11-06T22:47:59.189135Z","iopub.status.idle":"2025-11-06T22:47:59.200966Z","shell.execute_reply.started":"2025-11-06T22:47:59.189096Z","shell.execute_reply":"2025-11-06T22:47:59.199549Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Finding average duration of every label/exercise indicated by every set</h4>","metadata":{}},{"cell_type":"code","source":"# average duration of every set\nfor s in df[\"set\"].unique():\n    start    = df[df[\"set\"]==s].index[0]\n    stop     = df[df[\"set\"]==s].index[-1]\n    duration = stop - start\n\n    df.loc[ df[\"set\"]==s, \"duration\"] = duration.seconds\n\n\n# find the average duration based on category\nduration_df = df.groupby([\"category\"])[\"duration\"].mean() \nprint(duration_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:48:01.386016Z","iopub.execute_input":"2025-11-06T22:48:01.386334Z","iopub.status.idle":"2025-11-06T22:48:01.471337Z","shell.execute_reply.started":"2025-11-06T22:48:01.386313Z","shell.execute_reply":"2025-11-06T22:48:01.470307Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h3>ðŸ“š 4.2: Reduce noise/smoothing data </h3>\n\n<font size=\"3\" color=\"black\"> Let's look at two approaches in reducing the noise of accelermoter and gyroscope readings <br>\n\n<font size=\"3\" color=\"black\">  1. Butter worth low pass filter - <font color=\"royalblue\">Existing approach <br>\n    \n<font size=\"3\" color=\"black\">  2. Kalman filter - <font color=\"royalblue\"> Proposed Approach \n\n</font>","metadata":{}},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> \nâœ¨ Let's look at reducing noise using Butter worth low pass filter \n<font color=\"royalblue\">(Exisiting approach) </font>\n</h4>","metadata":{}},{"cell_type":"code","source":"# Butterworth lowpass filter\n\nfrom pathlib import Path\n\ndf_lowpass = df.copy()\nLowPass = LowPassFilter()\n\n# sample frequency\nfs = 1000/200\n\n#Lower cutoff value gives higher smoothness\ncut_off = 1.2\n\ncolumn = \"gyr_z\"\nbutterworth_column = \"gyr_z_lowpass\"\nset1 = 55\ndf_lowpass = LowPass.low_pass_filter(df_lowpass,column,fs,cut_off, order=5)\nsubset = df_lowpass[df_lowpass[\"set\"]==set1] \nfig , ax = plt.subplots(nrows=1, sharex=True, figsize=(20,5))\nplt.plot(subset[column].reset_index(drop=True), label = \"raw data/ Original data\")\nplt.plot(subset[butterworth_column].reset_index(drop=True), label = \"butter worth filter\")\nax.legend(loc=\"upper right\",bbox_to_anchor=(1.0,1.15), fancybox=True, shadow=True)\n#ax.legend(loc=\"upper right\",bbox_to_anchor=(0.5,1.15), fancybox=True, shadow=True)  \nplt.xlabel(\"Samples\")\nplt.ylabel(f\"{column}\")\nplt.title(f\"{column}\\nset : {set1}\")\nPath('/kaggle/working/Build_features/Butterworth_lowpass_filter').mkdir(parents=True, exist_ok=True)\nplt.savefig(f\"/kaggle/working/Build_features/Butterworth_lowpass_filter/{column}(set{set1}).png\")\n\ncolumn = \"acc_y\"\nbutterworth_column = \"acc_y_lowpass\"\nset2 = 25\ndf_lowpass = LowPass.low_pass_filter(df_lowpass,column,fs,cut_off, order=5)\nsubset = df_lowpass[df_lowpass[\"set\"]==set2] \nfig , ax = plt.subplots(nrows=1, sharex=True, figsize=(20,10))\nplt.plot(subset[column].reset_index(drop=True), label = \"raw data/Original reading\",color = \"blue\")\nplt.plot(subset[butterworth_column].reset_index(drop=True), label = \"butter worth filter\", color = \"orange\")\nax.legend(loc=\"upper right\",bbox_to_anchor=(1.0,1.15), fancybox=True, shadow=True)\n#ax.legend(loc=\"upper center\",bbox_to_anchor=(0.5,1.15), fancybox=True, shadow=True) \nplt.xlabel(\"Samples\")\nplt.ylabel(f\"{column}\")\nplt.title(f\"{column}\\nset : {set2}\")\nPath('/kaggle/working/Build_features/Butterworth_lowpass_filter').mkdir(parents=True, exist_ok=True)\nplt.savefig(f\"/kaggle/working/Build_features/Butterworth_lowpass_filter/{column}(set{set2}).png\")\n\n#ax[0].plot(subset[\"acc_y\"].reset_index(drop=True), label = \"raw data/Original reading\")\n#ax[1].plot(subset[\"acc_y_lowpass\"].reset_index(drop=True), label = \"butter worth filter\")\n#ax[0].legend(loc=\"upper center\",bbox_to_anchor=(0.5,1.15), fancybox=True, shadow=True)\n#ax[1].legend(loc=\"upper center\",bbox_to_anchor=(0.5,1.15), fancybox=True, shadow=True)  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Let's look at reducing noise using Kalman filter \n<font color=\"royalblue\">(Proposed approach) </font> \n</h4>","metadata":{}},{"cell_type":"code","source":"# Kalman filter\n# avoids sharp peaks and valleys\n# reduces noise when there's no data in the raw data\n#---------------------------------------------------------------\n\nfrom pathlib import Path\n\n\ndf_kalman = df.copy()\ncolumn = \"gyr_z\"\nset1 = 55\nsubset = df_kalman[df_kalman[\"set\"]==set1][column].reset_index(drop=True)\nkf = KalmanFilter(transition_matrices=[1],\n                  observation_matrices=[1],\n                  initial_state_mean=subset.iloc[0],\n                  initial_state_covariance=1,\n                  observation_covariance=1,\n                  transition_covariance=0.8)\n           \n        \n# Fit model and make predictions\nstate_means, _ = kf.filter(subset)\nstate_means = state_means.flatten()   \n        \n# Plot original data and Kalman filter predictions\n\n\nplt.plot(subset, label='raw data/ Original data',color=\"blue\")\nplt.plot(state_means, label='Kalman Filter Predictions',color=\"red\")\nplt.legend()\nplt.xlabel(\"Values\")\nplt.ylabel(f\"{column}\")\nplt.title(f\"{column}\\nset : {set1}\")\n#plt.show()\n#Path('/kaggle/working/Build_features/Kalman_filter').mkdir(parents=True, exist_ok=True)\n#plt.savefig(f\"/kaggle/working/Build_features/Kalman_filter/{column}(set{set1}).png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Comparing butter worth low pass filter and kalman filter</h4>\n<font size=\"3\"> \n* Butter worth low pass filter has noise compared to Kalman filter</br>\n* Kalman filter applies smoothing without changing the original data\n</font>","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\n# Butterworth lowpass filter\n#-----------------------------------------------------------------------------\ndf_lowpass = df.copy()\nLowPass = LowPassFilter()\n\n# sample frequency\nfs = 1000/200\n\n#Lower cutoff value gives higher smoothness\ncut_off = 1.2\n\ncolumn = \"gyr_z\"\nbutterworth_column = \"gyr_z_lowpass\"\nset1 = 55\ndf_lowpass = LowPass.low_pass_filter(df_lowpass,column,fs,cut_off, order=5)\nsubset_Butterworth = df_lowpass[df_lowpass[\"set\"]==set1] \n\n# Kalman filter\n#-------------------------------------------------------------------------------\ndf_kalman = df.copy()\nsubset_kalman = df_kalman[df_kalman[\"set\"]==set1][column].reset_index(drop=True)\nkf = KalmanFilter(transition_matrices=[1],\n                  observation_matrices=[1],\n                  initial_state_mean=subset_kalman.iloc[0],\n                  initial_state_covariance=1,\n                  observation_covariance=1,\n                  transition_covariance=0.8)\n           \n        \n# Fit model and make predictions\nstate_means, _ = kf.filter(subset_kalman)\nstate_means = state_means.flatten()    \n        \n\nfig , ax = plt.subplots(nrows=1, sharex=True, figsize=(20,5))\nplt.plot(subset_Butterworth[column].reset_index(drop=True), label = \"raw data/ Original data\")\nplt.plot(subset_Butterworth[butterworth_column].reset_index(drop=True), label = \"butter worth filter\",color = \"red\")\nplt.plot(state_means, label='Kalman Filter Predictions',color=\"black\")\nax.legend(loc=\"upper right\",bbox_to_anchor=(1.0,1.15), fancybox=True, shadow=True)\n#ax.legend(loc=\"upper right\",bbox_to_anchor=(0.5,1.15), fancybox=True, shadow=True)  \nplt.xlabel(\"Samples\")\nplt.ylabel(f\"{column}\")\nplt.title(f\"{column}\\nset : {set1}\")\n\n#Path('/kaggle/working/Build_features/Compare_Butterworth_&_Kalman').mkdir(parents=True, exist_ok=True)\n#plt.savefig(f\"/kaggle/working/Build_features/Compare_Butterworth_&_Kalman/{column}(set{set1}).png\") \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Apply Kalman filter to all the accelerometer and gyroscope readings and export the dataset</h4>","metadata":{}},{"cell_type":"code","source":"\n# Replace accelermometer and gyrosocpe column with the Kalman filter values\n#----------------------------------------------------------------------------\n\nfrom pathlib import Path\n\ndf_kalman = df.copy()\n\n#Replace variables with required values for below ones \n#predictor_columns = [\"acc_x\"]  \n#sets = [55]\n\nsave_plot = True # assign True if you want to save plot\nsets = df[\"set\"].unique()\nfor s in sets:\n    for col in predictor_columns:\n        \n        subset = df_kalman[df_kalman[\"set\"]==s][col].reset_index(drop=True)\n        #print(subset)\n\n\n        # Define Kalman filter model\n        # avoids sharp peaks and valleys\n        # reduces noise when there's no data in the raw data\n        kf = KalmanFilter(transition_matrices=[1],\n                          observation_matrices=[1],\n                          initial_state_mean=subset.iloc[0],\n                          initial_state_covariance=1,\n                          observation_covariance=1,\n                          transition_covariance=0.8)\n           \n        \n        # Fit model and make predictions\n        state_means, _ = kf.filter(subset)\n        state_means = state_means.flatten()\n        \n        df_kalman.loc[df_kalman['set'] == s, col] = state_means\n        # Plot original data and Kalman filter predictions\n        plt.plot(subset, label='raw data',color=\"blue\")\n        plt.plot(state_means, label='Kalman Filter Predictions',color=\"red\")\n        plt.legend()\n        plt.xlabel(\"Values\")\n        plt.ylabel(f\"{col}\")\n        plt.title(f\"{col}\\nset : {s}\")\n        plt.show()\n\n        if (save_plot):\n            Path('/kaggle/working/Build_features/Kalman_filter_All(Set_&_Column)').mkdir(parents=True, exist_ok=True)\n            plt.savefig(f\"/kaggle/working/Build_features/Kalman_filter_All(Set_&_Column)/{col}(set{s}).png\") \n\n#export the dataset\nPath('/kaggle/working/interim1').mkdir(parents=True, exist_ok=True)\ndf_kalman.to_pickle(\"/kaggle/working/interim1/01_kalman_filter_applied.pkl\")\n\n# compare the original data with the kalman filtered data\n#df[\"acc_y\"][:100].reset_index(drop=True).plot()\n#df_kalman[\"acc_y\"][:100].reset_index(drop=True).plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Apply butter worth low pass filter to accelerometer and gyroscope readings  and export the dataset</h4>","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\ndf_lowpass = df.copy()\n\n\n# defining new object of LowPassFilter() class\nLowPass = LowPassFilter()\n\nfs = 1000/200\n\ncut_off = 1.2\n\npredictor_columns = list(df.columns[:6])\n\nfor col in predictor_columns:\n    df_lowpass = LowPass.low_pass_filter(df_lowpass,col,fs,cut_off, order=5)\n    df_lowpass[col] = df_lowpass[col+\"_lowpass\"]\n    del df_lowpass[col+\"_lowpass\"]\n\n# export the dataset\nPath('/kaggle/working/interim1').mkdir(parents=True, exist_ok=True)\ndf_lowpass.to_pickle(\"/kaggle/working/interim1/01_butter_worth_lowpass_applied.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h3> ðŸ“š 4.3: Apply Principal component Analysis(PCA) to Reduce dimensionality of datasets</h3>\n<font size=\"3\">\n1. Kalman dataset<br>\n2. Butter worth low pass dataset\n</font>           \n             ","metadata":{}},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Let's Apply PCA to Kalman dataset</h4>","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca1 = PCA()\n# Principal component analysis PCA\n# This dataframe has undergone kalman filter\ndf_kalman = pd.read_pickle(\"/kaggle/input/interim1/01_kalman_filter_applied.pkl\")\npredictor_columns = list(df_kalman.columns[:6])\ndf_pca_kalman = df_kalman.copy()\n\n# define the object of the PrincipalComponentAnalysis class\npca = PrincipalComponentAnalysis()\n\n# decide the optimal number of principal components\npc_values = pca.determine_pc_explained_variance(df_pca_kalman, predictor_columns)  \n\n\n# visualize the elbow curve\nprint(\"PCA values are : \",pc_values)\nplt.figure(figsize=(10,10))\nplt.plot(range(1, len(predictor_columns)+1),pc_values)\nplt.xlabel(\"principal component number\")\nplt.ylabel(\"explained vaiance\")\nplt.show()  \n\n# --- Fit PCA ---\npca1.fit(df_pca_kalman[predictor_columns])\n# --- Determine loadings (feature contributions to PCs) ---\nloadings = pd.DataFrame(\n    pca1.components_.T,\n    columns=[f'PC{i+1}' for i in range(len(predictor_columns))],\n    index=predictor_columns\n)\nprint(\"\\nPCA Loadings (feature contributions to each PC):\")\nprint(loadings)\n\n# --- Select top PCs that explain most variance ---\n# Example: take first 2 PCs (you can adjust based on elbow curve)\ntop_n_pcs = 3\n\n# Average absolute contribution across selected PCs\nselected_loadings = loadings.iloc[:, :top_n_pcs]\nfeature_importance = selected_loadings.abs().mean(axis=1)\n\n# Rank features by their contribution\ntop_features = feature_importance.sort_values(ascending=False)\n#print(f\"\\nTop contributing features across top {top_n_pcs} PCs:\")\n#print(top_features)\n\n# --- Optional: Weight by explained variance ---\n#weights = explained_variance[:top_n_pcs]\n\nexplained_variance = pca1.explained_variance_ratio_\nweights = explained_variance[:top_n_pcs]\n\nweighted_importance = (selected_loadings.abs() * weights).sum(axis=1)\ntop_weighted_features = weighted_importance.sort_values(ascending=False)\nprint(f\"\\nTop features weighted by explained variance (top {top_n_pcs} PCs):\")\nprint(top_weighted_features)\n\n# --- Visualization ---\nplt.figure(figsize=(8,5))\ntop_weighted_features.plot(kind='barh')\nplt.title(f\"Feature Contributions to Top {top_n_pcs} Principal Components\")\nplt.xlabel(\"Weighted Contribution\")\nplt.ylabel(\"Feature\")\nplt.gca().invert_yaxis()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T00:15:20.975497Z","iopub.execute_input":"2025-11-07T00:15:20.975781Z","iopub.status.idle":"2025-11-07T00:15:21.258860Z","shell.execute_reply.started":"2025-11-07T00:15:20.975766Z","shell.execute_reply":"2025-11-07T00:15:21.258303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\n\n# from the above graph, we'll use the elbow technique, which is  the point at which the rate of change in variance diminishes. It is is 3, \n# Perform apply_pca function the dataframe has normalized values for the predictor columns [acc_x, acc_y, acc_z. gyr_x, gyr_y, gyr_z] \n# New columns added are \"pca_1\",\"pca_2\", \"pca_3\"\ndf_pca_kalman = pca.apply_pca(df_pca_kalman, predictor_columns, 3)\n\n\n#visualize the new pca columns for a particular set\n\nsubset = df_pca_kalman[df_pca_kalman[\"set\"]==35]\nprint(subset)\nsubset[[\"pca_1\",\"pca_2\", \"pca_3\" ]].plot()\n\n#export dataset\nPath('/kaggle/working/interim1').mkdir(parents=True, exist_ok=True)\ndf_pca_kalman.to_pickle(\"/kaggle/working/interim1/01_pca_kalman_applied.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T20:58:39.007768Z","iopub.execute_input":"2025-11-06T20:58:39.008108Z","iopub.status.idle":"2025-11-06T20:58:39.275638Z","shell.execute_reply.started":"2025-11-06T20:58:39.008081Z","shell.execute_reply":"2025-11-06T20:58:39.274684Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Let's  Apply PCA to butterworth dataset</h4>","metadata":{}},{"cell_type":"code","source":"df_butterworth = pd.read_pickle(\"/kaggle/input/interim1/01_butter_worth_lowpass_applied.pkl\")\n\n# Principal component analysis PCA\n# This dataframe has undergone Butterworth low-pass filter\n\npredictor_columns = list(df_butterworth.columns[:6])\ndf_pca_butterworth = df_butterworth.copy()\n\n# define the object of the PrincipalComponentAnalysis class\npca1 = PrincipalComponentAnalysis()\n\n# First, we should decide the optimal number of principal components\npc_values1 = pca1.determine_pc_explained_variance(df_pca_butterworth, predictor_columns)  \n\n\n# visualize the elbow curve\nprint(\"PCA values are : \",pc_values1)\nplt.figure(figsize=(10,10))\nplt.plot(range(1, len(predictor_columns)+1),pc_values1)\nplt.xlabel(\"principal component number\")\nplt.ylabel(\"explained vaiance\")\nplt.show()  \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\n\n# from the above graph, we'll use the elbow technique, which is  the point at which the rate of change in variance diminishes. It is is 3, \n# Perform apply_pca function the dataframe has normalized values for the predictor columns [acc_x, acc_y, acc_z. gyr_x, gyr_y, gyr_z] \n# New columns added are \"pca_1\",\"pca_2\", \"pca_3\"\ndf_pca_butterworth = pca1.apply_pca(df_pca_butterworth, predictor_columns, 3)\n\n\n#visualize the new pca columns for a particular set\n\nsubset1 = df_pca_butterworth[df_pca_butterworth[\"set\"]==35]\nprint(subset1)\nsubset1[[\"pca_1\",\"pca_2\", \"pca_3\" ]].plot()\n\n#export dataset\nPath('/kaggle/working/interim1').mkdir(parents=True, exist_ok=True)\ndf_pca_butterworth.to_pickle(\"/kaggle/working/interim1/01_pca_butterworth_applied.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h2> ðŸ“š 4.4: Sum of sqaures attribute</h2> \n<font size=\"3\"> <br>  \n* Quantify model variability/Model optimization<br>\n* Apply to <br>\n    1. Kalman dataset<br>\n    2. Butter worth low pass dataset\n</font>","metadata":{}},{"cell_type":"markdown","source":"* * * * * * * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4>âœ¨ Let's Apply Sum of sqaures attribute to Kalman dataset</h4>","metadata":{}},{"cell_type":"code","source":"# Sum of squares attributes\n# --------------------------------------------------------------\n# this gives scalar magnitude, which is impartial to the device orientation\n# helps the model to generalize better to different participants\n\nfrom IPython.display import display\nimport pandas as pd\nimport numpy as np\n\ndf_kalman_new = pd.read_pickle(\"/kaggle/input/interim1/01_pca_kalman_applied.pkl\")\n\ndf_ss_kalman = df_kalman_new.copy()\nacc_r      = df_ss_kalman[\"acc_x\"]**2 + df_ss_kalman[\"acc_y\"]**2 + df_ss_kalman[\"acc_z\"]**2\ngyr_r      = df_ss_kalman[\"gyr_x\"]**2 + df_ss_kalman[\"gyr_y\"]**2 + df_ss_kalman[\"gyr_z\"]**2\n\ndf_ss_kalman[\"acc_r\"] = np.sqrt(acc_r)\ndf_ss_kalman[\"gyr_r\"] = np.sqrt(gyr_r)\nprint(\"Sum of sqaured attributes are\".title())\ndisplay(df_ss_kalman[[\"acc_r\",\"gyr_r\"]])\n\n\nset1 = 50\nprint(f\"Sum of sqaured attributes set : {set1}\")\nsubset = df_ss_kalman[df_ss_kalman[\"set\"]==set1].reset_index()\nsubset[[\"acc_r\",\"gyr_r\" ]].plot(subplots=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:05:17.048394Z","iopub.execute_input":"2025-11-06T22:05:17.048724Z","iopub.status.idle":"2025-11-06T22:05:17.306821Z","shell.execute_reply.started":"2025-11-06T22:05:17.048700Z","shell.execute_reply":"2025-11-06T22:05:17.305883Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4>âœ¨ Let's Apply Sum of sqaures attribute to butter worth dataset</h4>","metadata":{}},{"cell_type":"code","source":"# Sum of squares attributes\n# --------------------------------------------------------------\n# this gives scalar maginitude, which is impartial to the device orientation\n# helps the model to generalize better to different participants\n\nfrom IPython.display import display\nimport pandas as pd\nimport numpy as np\n\ndf_butterworth_new = pd.read_pickle(\"/kaggle/input/interim1/01_pca_butterworth_applied.pkl\")\n\ndf_ss_butterworth = df_butterworth_new.copy()\nacc_r      = df_ss_butterworth[\"acc_x\"]**2 + df_ss_butterworth[\"acc_y\"]**2 + df_ss_butterworth[\"acc_z\"]**2\ngyr_r      = df_ss_butterworth[\"gyr_x\"]**2 + df_ss_butterworth[\"gyr_y\"]**2 + df_ss_butterworth[\"gyr_z\"]**2\n\ndf_ss_butterworth[\"acc_r\"] = np.sqrt(acc_r)\ndf_ss_butterworth[\"gyr_r\"] = np.sqrt(gyr_r)\nprint(\"Sum of sqaured attributes are\".title())\ndisplay(df_ss_butterworth[[\"acc_r\",\"gyr_r\"]])\n\n\nset1 = 50\nprint(f\"Sum of sqaured attributes set : {set1}\")\nsubset = df_ss_butterworth[df_ss_butterworth[\"set\"]==set1].reset_index()\nsubset[[\"acc_r\",\"gyr_r\" ]].plot(subplots=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h3> ðŸ“š 4.5: Temporal Abstraction </h3>\n<font size=\"3\"> <br>\n* Smooth out time series or sequential data by taking the average of a fixed-size sliding window over your data <br>\n* Apply to <br>               \n    1. Kalman dataset<br>\n    2. Butter worth low pass dataset\n</font>","metadata":{}},{"cell_type":"markdown","source":"* * * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Let's Apply temporal abstraction to kalman dataset</h4>","metadata":{}},{"cell_type":"code","source":"# Temporal abstraction\n# --------------------------------------------------------------\nfrom pathlib import Path\n\ndf_temp_kalman = df_ss_kalman.copy()\nNumAbs = NumericalAbstraction()\n\n# predictor columns are already added in the beginning as predictor columns[\"acc_x\", \"acc_y\", \"acc_z\", \"gyr_x\", \"gyr_y\", \"gyr_z\"]\n# let's add new columns [\"acc_r\" , \"gyr_r\"] to it\n\npredictor_columns = predictor_columns + [\"acc_r\" , \"gyr_r\"]\n\n# choosing the optimum window size is trial and error method\n# if we use window size 5 => we can't use first 4 values = > windows size is taking current value and previous 4 values, altogether 5 values \n# currently each row of dataframe is for 200ms,  we want to have each row of data frame for 1s or 1000ms => so for that we need window size 1000/200 = 5\n\nws = int(1000/200)\n\n\ndf_temp_list = []\n\nfor s in df_temp_kalman[\"set\"].unique():\n    subset = df_temp_kalman[df_temp_kalman[\"set\"]==s].copy()\n    for col in predictor_columns:\n        subset = NumAbs.abstract_numerical(subset, [col], ws, \"mean\")\n        subset = NumAbs.abstract_numerical(subset, [col], ws, \"std\")\n    \n    df_temp_list.append(subset)\n\ndf_temporal_kalman = pd.concat(df_temp_list) \n\n\nset1 = 71\nprint(f\"Plot for set {set1} \")\nset = df_temporal_kalman[df_temporal_kalman[\"set\"] == set1]\nset[[\"acc_y\",\"acc_y_temp_mean_ws_5\",\"acc_y_temp_std_ws_5\"]].plot()\n\n\n# export dataset\n#--------------------------------------------------------------------------------\nPath('/kaggle/working/interim1').mkdir(parents=True, exist_ok=True)\ndf_temporal_kalman.to_pickle(\"/kaggle/working/interim1/01_temporal_abstraction_kalman_applied.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T23:20:55.562096Z","iopub.execute_input":"2025-11-06T23:20:55.562412Z","iopub.status.idle":"2025-11-06T23:21:14.026307Z","shell.execute_reply.started":"2025-11-06T23:20:55.562391Z","shell.execute_reply":"2025-11-06T23:21:14.025606Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Display Kalman dataset with temporal abstraction </h4>","metadata":{}},{"cell_type":"code","source":"df_new = pd.read_pickle(\"/kaggle/input/interim1/01_temporal_abstraction_kalman_applied.pkl\")\nfreq_features = [f for f in df_new.columns if (\"temp\" in f)]\nprint(\"\\nNumber of frequency features are : \",len(freq_features))\nprint(\"\\nFrequency features are :\\n\\n \", freq_features)\nprint(\"\\n\\nDisplying dataframe :\\n\", df_new[freq_features])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h3> âœ¨ Let's Apply temporal abstraction to Butterworth dataset</h3>","metadata":{}},{"cell_type":"code","source":"# Temporal abstraction\n# --------------------------------------------------------------\nfrom pathlib import Path\n\ndf_temp_butterworth = df_ss_butterworth.copy()\nNumAbs = NumericalAbstraction()\n\n# predictor columns are already added in the beginning as predictor columns[\"acc_x\", \"acc_y\", \"acc_z\", \"gyr_x\", \"gyr_y\", \"gyr_z\"]\n# let's add new columns [\"acc_r\" , \"gyr_r\"] to it\n\npredictor_columns = predictor_columns + [\"acc_r\" , \"gyr_r\"]\n\n# choosing the optimum window size is trial and error method\n# if we use window size 5 => we can't use first 4 values = > windows size is taking current value and previous 4 values, altogether 5 values \n# currently each row of dataframe is for 200ms,  we want to have each row of data frame for 1s or 1000ms => so for that we need window size 1000/200 = 5\n\nws = int(1000/200)\n\ndf_temp_list = []\n\nfor s in df_temp_butterworth[\"set\"].unique():\n    subset = df_temp_butterworth[df_temp_butterworth[\"set\"]==s].copy()\n    for col in predictor_columns:\n        subset = NumAbs.abstract_numerical(subset, [col], ws, \"mean\")\n        subset = NumAbs.abstract_numerical(subset, [col], ws, \"std\")\n    \n    df_temp_list.append(subset)\n\ndf_temporal_butterworth = pd.concat(df_temp_list) \n\n\nset1 = 71\nprint(f\"Plot for set {set1} \")\nset = df_temporal_butterworth[df_temporal_butterworth[\"set\"] == set1]\nset[[\"acc_y\",\"acc_y_temp_mean_ws_5\",\"acc_y_temp_std_ws_5\"]].plot()\n\n\n# export dataset\n#--------------------------------------------------------------------------------\nPath('/kaggle/working/interim1').mkdir(parents=True, exist_ok=True)\ndf_temporal_butterworth.to_pickle(\"/kaggle/working/interim1/01_temporal_abstraction_butter_worth_applied.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T22:03:54.586185Z","iopub.execute_input":"2025-11-06T22:03:54.586720Z","iopub.status.idle":"2025-11-06T22:03:54.653278Z","shell.execute_reply.started":"2025-11-06T22:03:54.586699Z","shell.execute_reply":"2025-11-06T22:03:54.651267Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4>âœ¨ Display Butter worth dataset with temporal abstraction </h4>","metadata":{}},{"cell_type":"code","source":"df_new = pd.read_pickle(\"/kaggle/input/interim1/01_temporal_abstraction_butter_worth_applied.pkl\")\nfreq_features = [f for f in df_new.columns if (\"temp\" in f)]\nprint(\"\\nNumber of frequency features are : \",len(freq_features))\nprint(\"\\nFrequency features are :\\n\\n \", freq_features)\nprint(\"\\n\\nDisplying dataframe :\\n\", df_new[freq_features])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h3>ðŸ“š 4.6: Frequency abstraction</h3>\n<font size=\"3\"> <br>\nLet's analyse the two different approaches of frequency abstraction<br>\n    1. Fourier transformation<br>\n    2. Variational mode decomposition","metadata":{}},{"cell_type":"markdown","source":"* * * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Lets look at frequency abstraction through Fourier transformation</h4>","metadata":{}},{"cell_type":"code","source":"# Fourier transformation\n#------------------------------------------------\nfrom pathlib import Path\n\ndf_butterworth_new = pd.read_pickle(\"/kaggle/input/interim1/01_temporal_abstraction_butter_worth_applied.pkl\")\ndf_freq = df_butterworth_new.copy().reset_index()\nfreqAbs = FourierTransformation()\n\nfs = int(1000/200)\n\n# window size\nws = int(2800/200)\n\n\n#loop over each set and perform fourier transformation\ndf_freq_list = []\nfor s in df_freq[\"set\"].unique():\n    subset = df_freq[df_freq[\"set\"]==s].reset_index(drop=True).copy()\n    for col in predictor_columns:\n        subset = freqAbs.abstract_frequency(subset, [col], ws, fs)\n        df_freq_list.append(subset)\n\ndf_freq = pd.concat(df_freq_list).set_index(\"epoch (ms)\",drop=True)\n\nfreq_features = [f for f in df_freq.columns if (\"freq\" in f)]\nprint(df_freq[freq_features])\n\n# export dataset\n#--------------------------------------------------------------------------------\nPath('/kaggle/working/interim1').mkdir(parents=True, exist_ok=True)\ndf_freq.to_pickle(\"/kaggle/working/interim1/01_butter_worth_fourier_transformation_applied.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Display dataframe with fourier transformation</h4>","metadata":{}},{"cell_type":"code","source":"from IPython.display import display\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf_fourier = pd.read_pickle(\"/kaggle/input/interim1/01_butter_worth_fourier_transformation_applied.pkl\")\nfreq_features = [f for f in df_fourier.columns if (\"freq\" in f)]\nprint(\"\\nlength of newly created columns : \",len(freq_features),\"\\n\\n\")\ndisplay(df_fourier[freq_features])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Lets look at frequency abstraction through Variational mode decomposition</h4>","metadata":{}},{"cell_type":"code","source":"# Frequency features\n# --------------------------------------------------------------\n#Applying variational mode decomposition\nimport pandas as pd\nfrom pathlib import Path\n\ndf_freq = pd.DataFrame() \n\ndf_new = pd.read_pickle(\"/kaggle/input/interim1/01_temporal_abstraction_kalman_applied.pkl\")\ndf_freq = df_new.copy().reset_index()\nfreqAbs1 = Variational_Mode_Decomposition\n\n# Loop over each unique \"set\"\nfor s in df_freq[\"set\"].unique():\n    subset = df_freq[df_freq[\"set\"] == s].reset_index(drop=True)  # reset index for safe VMD\n    mask = df_freq['set'] == s\n    n_target = mask.sum()  # number of rows to fill in df_freq\n\n    for col in predictor_columns:\n        # Run VMD on the column\n        subset1 = subset[col]\n        subset_vmd = freqAbs1.vmd(subset1, col, 8)\n        subset_vmd = subset_vmd  # reset index\n\n        n_vmd = len(subset_vmd)  # number of rows from VMD output\n\n        # Ensure lengths match\n        if n_vmd < n_target:\n            # pad with NaN\n            subset_vmd = subset_vmd.reindex(range(n_target))\n        elif n_vmd > n_target:\n            # truncate extra rows\n            subset_vmd = subset_vmd.iloc[:n_target]\n\n        # Assign safely back to df_freq\n        for column in subset_vmd.columns:\n            df_freq.loc[mask, column] = subset_vmd[column].values\n\ndf_freq = df_freq.set_index(\"epoch (ms)\",drop=True)\n# export dataset\n#--------------------------------------------------------------------------------\nPath('/kaggle/working/interim1').mkdir(parents=True, exist_ok=True)\ndf_freq.to_pickle(\"/kaggle/working/interim1/01_Kalman_filter_Variational_mode_decomposition_applied.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T23:19:33.976914Z","iopub.execute_input":"2025-11-06T23:19:33.977215Z","iopub.status.idle":"2025-11-06T23:19:52.814322Z","shell.execute_reply.started":"2025-11-06T23:19:33.977196Z","shell.execute_reply":"2025-11-06T23:19:52.813407Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Display dataframe with Variational Mode decomposition(VMD)</h4>","metadata":{}},{"cell_type":"code","source":"from IPython.display import display\n\n#df_VMD = pd.read_pickle(\"/kaggle/input/interim1/01_VMD_applied.pkl\")\ndf_VMD = pd.read_pickle(\"/kaggle/input/interim1/01_Kalman_filter_Variational_mode_decomposition_applied.pkl\")\nfreq_features = [f for f in df_VMD.columns if (\"VMD\" in f)]\nprint(\"\\n\\nlength of newly created VMD columns : \",len(freq_features),\"\\n\\n\")\ndisplay(df_VMD)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T23:57:57.571773Z","iopub.execute_input":"2025-11-06T23:57:57.572054Z","iopub.status.idle":"2025-11-06T23:57:57.614623Z","shell.execute_reply.started":"2025-11-06T23:57:57.572037Z","shell.execute_reply":"2025-11-06T23:57:57.613669Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * * * * ***   ","metadata":{}},{"cell_type":"markdown","source":"<h3> 4.7: ðŸ“šDealing with overlapping windows</h3>\n<font size=\"3\">  <br>\nDealing with overlapping windows for below datasets<br>\n    1. Dataset with Variational mode decomposition/VMD <br>\n    2. Dataset with Fast fourier transformation/(fft)<br>\n</font>","metadata":{}},{"cell_type":"code","source":"# --------------------------------------------------------------\n# Dealing with overlapping windows\n# --------------------------------------------------------------\n\nfrom IPython.display import display\nimport pandas as pd\nfrom pathlib import Path\n\ndf_fourier = pd.read_pickle(\"/kaggle/input/interim1/01_butter_worth_fourier_transformation_applied.pkl\")\ndf_VMD = pd.read_pickle(\"/kaggle/input/interim1/01_Kalman_filter_Variational_mode_decomposition_applied.pkl\")\n\ndf_fourier = df_fourier.dropna()\ndf_VMD = df_VMD.dropna()\n\n# values between every rows are correlated or overlap after we did the rolling function.\n# we should avoid them when building model as this may result in overfitting\n# we should allow certain percentage of overlap while removing the rest of the data\n# getting rid of 50% of data is recommended- we can do by skipping every other row\n\ndf_fourier = df_fourier.iloc[::2] \ndf_VMD = df_VMD.iloc[::2]\n\ndisplay(df_fourier)\ndisplay(df_VMD)\n\n#export dataset\n#--------------------------------------------------------------------------------------\nPath('/kaggle/working/interim1').mkdir(parents=True, exist_ok=True)\ndf_fourier.to_pickle(\"/kaggle/working/interim1/01_Train_model_fourier.pkl\")\ndf_VMD.to_pickle(\"/kaggle/working/interim1/01_Train_model_VMD.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* * * ***   ","metadata":{}},{"cell_type":"markdown","source":"\n> # ðŸŒˆ Step 5 : Predictive modelling","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Split features into different subsets</h4>\n<h4> âœ¨ Carry out forward feature selection utilizing decision tree and identify the best 10 features and their scores</h4>\n<h4> âœ¨ Grid search Across different machine learning Approaches </h4>","metadata":{}},{"cell_type":"markdown","source":"<h3> ðŸ“š Apply predictive modelling to Dataset with Fourier transformation </h3>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom learningalgorithms1 import ClassificationAlgorithms\nimport seaborn as sns\nimport itertools\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Plot settings\nplt.style.use(\"fivethirtyeight\")\nplt.rcParams[\"figure.figsize\"] = (20, 5)\nplt.rcParams[\"figure.dpi\"] = 100\nplt.rcParams[\"lines.linewidth\"] = 2\n\n\n\n# apply to datframe with fourier transform\ndf = pd.read_pickle(\"/kaggle/input/interim1/01_Train_model_fourier.pkl\")\n\n\n# Create a training and test set\n# drop unnecessary columns\ndf_train  = df.drop([\"participant\", \"category\", \"set\",\"duration\"],axis=1)\n\n#split data frame into training and test set\n\nX = df_train.drop(\"label\",axis=1)\ny = df_train[\"label\"]\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h4> âœ¨ Split features into different subsets</h4>","metadata":{}},{"cell_type":"code","source":"basic_features = [\"acc_x\",\"acc_y\",\"acc_z\",\"gyr_x\",\"gyr_y\",\"gyr_z\"]\nsqaure_features = [\"acc_r\", \"gyr_r\"]\npca_features = [\"pca_1\",\"pca_2\",\"pca_3\"]\ntime_features = [f for f in df_train.columns if (\"_temp_\" in f)]\nfreq_features = [f for f in df_train.columns if (\"freq\" in f) or (\"_pse\" in f)]\n\n\nfeature_set_1 = list(basic_features)\nfeature_set_2 = list(basic_features + sqaure_features + pca_features )\nfeature_set_3 = list(feature_set_2 + time_features )\nfeature_set_4 = list(feature_set_3 + freq_features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h4> âœ¨ Carry out forward feature selection utilizing decision tree and identify the best 10 features and their scores</h4>","metadata":{}},{"cell_type":"code","source":"# Perform forward feature selection using a simple decision tree\n# Select 10 best performing features and create another subset which will be feature_set_5\n#create an object of ClassificationAlgorithms\n\nfeatures_exist = True\nlearner = ClassificationAlgorithms()\nmax_features  = 10\n\nif(features_exist):\n    print(\"Forward feature selection completed\")\n\nelse :\n    # this will select the best 10 features of the dataframe that have high performance\n    selected_features, ordered_features, ordered_scores = learner.forward_selection(max_features, X_train, y_train)\n    print(\"\\nselected_features : \\n\", selected_features)\n    print(\"\\nordered_scores : \\n\", ordered_scores)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h4> âœ¨ Noted down the best 10 features and their scores </h4>","metadata":{}},{"cell_type":"code","source":"selected_features = [\n    'acc_z_freq_0.0_Hz_ws_14', \n    'acc_x_freq_0.0_Hz_ws_14', \n    'gyr_r_freq_0.0_Hz_ws_14', \n    'acc_z', \n    'acc_y_freq_0.0_Hz_ws_14', \n    'gyr_r_freq_1.429_Hz_ws_14', \n    'gyr_x_freq_0.714_Hz_ws_14', \n    'acc_y_max_freq', \n    'acc_z_freq_1.071_Hz_ws_14', \n    'acc_r_max_freq'\n]\n\n\n\nordered_scores = [\n    0.885556704584626, \n    0.9886246122026887, \n    0.9955187866253016, \n    0.9975870389520854, \n    0.9982764563943468, \n    0.9993105825577387, \n    0.9993105825577387, \n    0.9993105825577387, \n    0.9993105825577387, \n    0.9993105825577387\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h4> âœ¨ Grid search Across different machine learning Approaches </h4>\n<font size=\"3\"> <br>\n* <font color=\"black\">Grid search through approaches such as <font color=\"Blue\"> \n   Neural Network, Random Forest, K Nearest Neighbor, Naive Bayes and Decision Tree <br> \n    <font color=\"black\">\n*  Export the dataset of different approaches and their corresponding scores\n</font>","metadata":{}},{"cell_type":"code","source":"\n# Goes through all possible combinations of the parameters \n# Evaluates the modelâ€™s performance for each combination (through cross-validation)\n\nfrom pathlib import Path\n\n\npossible_feature_sets = [\n        feature_set_1,\n        feature_set_2, \n        feature_set_3,\n        feature_set_4,\n        selected_features   \n    ]\n    \n    \nfeature_names = [\n    \n        \"Feature Set 1\",\n        \"Feature Set 2\",\n        \"Feature Set 3\",\n        \"Feature Set 4\",\n        \"Selected Features\"\n        \n    ]\n\n\n\n\nif (Path(\"/kaggle/input/interim1/01_score_df.pkl\")).is_file():\n    print(\"Grid search completed\")\n\nelse :\n    print(\"Performing Grid search.............\")\n    iterations = 1\n    score_df = pd.DataFrame()\n    \n    \n    for i, f in zip(range(len(possible_feature_sets)), feature_names):\n        print(\"Feature set:\", i)\n        selected_train_X = X_train[possible_feature_sets[i]]\n        selected_test_X = X_test[possible_feature_sets[i]]\n    \n        # First run non deterministic classifiers to average their score.\n        performance_test_nn = 0\n        performance_test_rf = 0\n    \n        for it in range(0, iterations):\n            print(\"\\tTraining neural network,\", it)\n            (\n                class_train_y,\n                class_test_y,\n                class_train_prob_y,\n                class_test_prob_y,\n            ) = learner.feedforward_neural_network(\n                selected_train_X,\n                y_train,\n                selected_test_X,\n                gridsearch=False,\n            )\n            performance_test_nn += accuracy_score(y_test, class_test_y)\n    \n            print(\"\\tTraining random forest,\", it)\n            (\n                class_train_y,\n                class_test_y,\n                class_train_prob_y,\n                class_test_prob_y,\n            ) = learner.random_forest(\n                selected_train_X, y_train, selected_test_X, gridsearch=True\n            )\n            performance_test_rf += accuracy_score(y_test, class_test_y)\n    \n        performance_test_nn = performance_test_nn / iterations\n        performance_test_rf = performance_test_rf / iterations\n    \n      \n        print(\"\\tTraining KNN\")\n        (\n            class_train_y,\n            class_test_y,\n            class_train_prob_y,\n            class_test_prob_y,\n        ) = learner.k_nearest_neighbor(\n            selected_train_X, y_train, selected_test_X, gridsearch=True\n        )\n        performance_test_knn = accuracy_score(y_test, class_test_y)\n    \n        print(\"\\tTraining decision tree\")\n        (\n            class_train_y,\n            class_test_y,\n            class_train_prob_y,\n            class_test_prob_y,\n        ) = learner.decision_tree(\n            selected_train_X, y_train, selected_test_X, gridsearch=True\n        )\n        performance_test_dt = accuracy_score(y_test, class_test_y)\n    \n        print(\"\\tTraining naive bayes\")\n        (\n            class_train_y,\n            class_test_y,\n            class_train_prob_y,\n            class_test_prob_y,\n        ) = learner.naive_bayes(selected_train_X, y_train, selected_test_X)\n    \n        performance_test_nb = accuracy_score(y_test, class_test_y)\n    \n        # Save results to dataframe\n        models = [\"NN\", \"RF\", \"KNN\", \"DT\", \"NB\"]\n        new_scores = pd.DataFrame(\n            {\n                \"model\": models,\n                \"feature_set\": f,\n                \"accuracy\": [\n                    performance_test_nn,\n                    performance_test_rf,\n                    performance_test_knn,\n                    performance_test_dt,\n                    performance_test_nb,\n                ],\n            }\n        )\n        score_df = pd.concat([score_df, new_scores])\n    \n    #export the dataset\n    score_df.to_pickle(\"/kaggle/working/01_score_df.pkl\") \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h4> âœ¨ Visualize the models and their scores </h4>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nscore_df = pd.read_pickle(\"/kaggle/input/interim1/01_score_df.pkl\")\n\nprint(score_df.sort_values(by=\"accuracy\", ascending=False))\n\n\nplt.figure(figsize=(10,10))\nsns.barplot(x='model',y='accuracy',hue='feature_set',data=score_df)\nplt.xlabel(\"Model\")\nplt.ylabel(\"Accuracy\")\nplt.ylim(0.7,1)\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T23:39:56.378950Z","iopub.execute_input":"2025-11-06T23:39:56.379323Z","iopub.status.idle":"2025-11-06T23:39:57.009538Z","shell.execute_reply.started":"2025-11-06T23:39:56.379300Z","shell.execute_reply":"2025-11-06T23:39:57.008403Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> # ðŸŒˆ Step 6:  Model Evaluation and Validation","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Evaluating Model based on exisiting Best approach (Random Forest) </h4>","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.datasets import load_iris\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport itertools\n\n\nrf = RandomForestClassifier(\n    n_estimators=100,   # number of trees\n    max_depth=3,     # let the trees expand until all leaves are pure\n    random_state=42,    # for reproducibility\n    n_jobs=-1           # use all CPU cores for faster training\n)\n\n# Train the model\nrf.fit(X_train, y_train)\n\n#Predict on the test set\ny_pred = rf.predict(X_test)\npred_prob_test_y = rf.predict_proba(X_test)\nclass_test_prob_y = pd.DataFrame(pred_prob_test_y, columns=rf.classes_)\n\nclasses = class_test_prob_y.columns\n\ncm = confusion_matrix(y_test, y_pred, labels=classes)\n\n# create confusion matrix\nplt.figure(figsize=(10, 10))\nplt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\nplt.title(\"Confusion matrix\")\nplt.colorbar()\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\n\nthresh = cm.max() / 2.0\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    plt.text(\n        j,\n        i,\n        format(cm[i, j]),\n        horizontalalignment=\"center\",\n        color=\"white\" if cm[i, j] > thresh else \"black\",\n    )\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.grid(False)\nplt.show() \n\nfrom sklearn.metrics import f1_score, classification_report\n\n# Evaluate performance\nprint(\"âœ… Accuracy:\", accuracy_score(y_test, y_pred))\n\n# --- Compute F1 scores ---\nmacro_f1 = f1_score(y_test, y_pred, average='macro')\nmicro_f1 = f1_score(y_test, y_pred, average='micro')\nweighted_f1 = f1_score(y_test, y_pred, average='weighted')\n\nprint(\"\\nF1 Scores:\")\nprint(f\"Macro-F1:    {macro_f1:.4f}\")\nprint(f\"Micro-F1:    {micro_f1:.4f}\")\nprint(f\"Weighted-F1: {weighted_f1:.4f}\")\n\n\nprint(\"\\nDetailed classification report:\")\nprint(\"\\nðŸ“Š Classification Report:\\n\", classification_report(y_test, y_pred))\n\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"  >  # ðŸŒˆ Proposed Approach CNN + BiLSTM + Attention with Augmentation","metadata":{}},{"cell_type":"markdown","source":"<h4> âœ¨ Install required dependencies for BiLSTM approach </h4>","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --force-reinstall \\\n    numpy==1.26.4 \\\n    pandas==2.2.2 \\\n    matplotlib==3.8.4 \\\n    scipy==1.13.1 \\\n    scikit-learn==1.5.2 \\\n    tensorflow==2.18.0 \\\n    jax==0.4.34 \\\n    jaxlib==0.4.34 \\\n    google-cloud-bigquery==3.31.0 \\\n    google-cloud-bigquery-storage==2.30.0 \\\n    protobuf==4.25.3 \\\n    --no-cache-dir\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":">  # ðŸŒˆ Train model\n\n<h4> âœ¨ CNN + BiLSTM + Attention with Augmentation </h4>","metadata":{}},{"cell_type":"code","source":"# =============================================\n# CNN + BiLSTM + Attention+Augmentation with Sliding Windows\n# =============================================\nimport os, random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, classification_report\nfrom scipy.stats import mode\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom kerastuner import HyperParameters\nfrom kerastuner.tuners import RandomSearch\nfrom pathlib import Path\n\n\n# --------------------------\n# Fix seeds for reproducibility\n# --------------------------\nSEED = 42\nos.environ['PYTHONHASHSEED'] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\n# --------------------------\n# Load Data\n# --------------------------\ndf = pd.read_pickle(\"/kaggle/input/interim1/01_Train_model_VMD.pkl\")  \n\n# Original IMU feature columns\nimu_cols = [c for c in df.columns if c not in [\"participant\", \"category\", \"set\",\"duration\",\"epoch (ms)\",\"label\"]]\n\n# --------------------------\n# Add derived features (optional but helps)\n# --------------------------\n\n\nfeature_cols = imu_cols \n\nX_all = df[feature_cols].values.astype('float32')\ny_all = df['label'].values\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_all_encoded = label_encoder.fit_transform(y_all)\nprint(\"Label mapping:\", dict(zip(label_encoder.classes_, y_all_encoded)))\n\n# --------------------------\n# Create sequences using sliding windows with majority label\n# --------------------------\nwindow_size = 50  # timesteps per sequence\nstep = 25        # higher overlap\nX_sequences = []\ny_sequences = []\n\nfor start in range(0, len(X_all) - window_size + 1, step):\n    end = start + window_size\n    X_seq = X_all[start:end]\n    # majority label in window\n   \n    y_seq = int(mode(y_all_encoded[start:end], keepdims=True).mode[0])\n    \n    X_sequences.append(X_seq)\n    y_sequences.append(y_seq)\n\nX_sequences = np.array(X_sequences)\ny_sequences = np.array(y_sequences)\n\nprint(\"Total sequences:\", X_sequences.shape[0], \"Sequence shape:\", X_sequences.shape[1:])\n\n# --------------------------\n# Train/Test split\n# --------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X_sequences, y_sequences, test_size=0.2, stratify=y_sequences, random_state=SEED\n)\n\n# --------------------------\n# Compute class weights\n# --------------------------\nclasses = np.unique(y_train)\ncw = class_weight.compute_class_weight('balanced', classes=classes, y=y_train)\nclass_weights = {int(c): float(w) for c, w in zip(classes, cw)}\n\n# --------------------------\n# Normalize features\n# --------------------------\nn_features = X_train.shape[2]\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.reshape(-1, n_features)).reshape(X_train.shape)\nX_test_scaled  = scaler.transform(X_test.reshape(-1, n_features)).reshape(X_test.shape)\n\n# --------------------------\n# On-the-Fly Augmentation\n# --------------------------\ndef augment_sequence_tf(seq):\n    seq = tf.cast(seq, tf.float32)\n    noise = tf.random.normal(shape=tf.shape(seq), mean=0.0, stddev=0.01)\n    seq = seq + noise\n    scale = tf.random.uniform(shape=[], minval=0.95, maxval=1.05)\n    seq = seq * scale\n    return seq\n\ndef build_dataset(X, y, batch_size=32, training=True):\n    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n    if training:\n        dataset = dataset.shuffle(buffer_size=len(X), seed=SEED)\n        dataset = dataset.map(lambda seq, label: (augment_sequence_tf(seq), label),\n                              num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\n\ntrain_dataset = build_dataset(X_train_scaled, y_train, batch_size=32, training=True)\nval_dataset   = build_dataset(X_test_scaled, y_test, batch_size=32, training=False)\n\n\n\nif (Path(\"/kaggle/input/modelset/deep_learning_model_Final.h5\")).is_file():\n    print(\"There is an existing model...\")\n    best_model = tf.keras.models.load_model(\"/kaggle/input/modelset/deep_learning_model_Final.h5\")\n\n\nelse:\n    # --------------------------\n    # Model Builder (CNN + BiLSTM + Attention)\n    # --------------------------\n    \n    \n    def build_model_imu(hp):\n        inp = layers.Input(shape=(window_size, n_features))\n        x = layers.Masking(mask_value=0.0)(inp)\n        x = layers.LayerNormalization()(x)\n        \n        # CNN residual blocks\n        for i in range(2):\n            filters = hp.Int(f'conv{i}_filters', 128, 256, step=64)\n            kernel = hp.Choice(f'conv{i}_kernel', [3,5])\n            x_prev = x\n            x = layers.Conv1D(filters, kernel, padding='same', activation='relu')(x)\n            x = layers.BatchNormalization()(x)\n            if x_prev.shape[-1] != filters:\n                x_res = layers.Conv1D(filters, 1, padding='same')(x_prev)\n            else:\n                x_res = x_prev\n            x = layers.Add()([x, x_res])\n            x = layers.MaxPooling1D(2)(x)\n        \n        # BiLSTM\n        lstm_units = hp.Int('lstm_units', 128, 256, step=64)\n        x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True, dropout=0.2))(x)\n        \n        # Optional MultiHead Attention\n        use_attn = hp.Choice('use_attention', [True, False])\n        if use_attn:\n            attn_heads = hp.Choice('attn_heads', [2, 4])\n            key_dim = hp.Choice('attn_key_dim', [16, 32])\n            attn = layers.MultiHeadAttention(num_heads=attn_heads, key_dim=key_dim)(x, x)\n            x = layers.Add()([x, attn])\n            x = layers.LayerNormalization()(x)\n        \n        x = layers.GlobalAveragePooling1D()(x)\n        \n        # Dense classifier\n        dense_units = hp.Int('dense_units', 128, 256, step=64)\n        x = layers.Dense(dense_units, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n        x = layers.Dropout(hp.Float('dropout', 0.2, 0.4, step=0.1))(x)\n        out = layers.Dense(len(np.unique(y_all_encoded)), activation='softmax')(x)\n        \n        lr = hp.Choice('learning_rate', [1e-3, 5e-4, 1e-4])\n        model = models.Model(inputs=inp, outputs=out)\n        model.compile(optimizer=tf.keras.optimizers.Adam(lr), \n                      loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        return model\n    \n    # --------------------------\n    # Hyperparameter Tuning\n    # --------------------------\n    tuner = RandomSearch(\n        build_model_imu,\n        objective='val_accuracy',\n        max_trials=20,\n        executions_per_trial=1,\n        directory='tuner_vmd',\n        project_name='imu_vmd_tuned'\n    )\n    \n    tuner.search(train_dataset, validation_data=val_dataset, epochs=50,\n                 class_weight=class_weights,\n                 callbacks=[EarlyStopping(patience=8, restore_best_weights=True),\n                            ReduceLROnPlateau(factor=0.5, patience=4, min_lr=1e-6)],\n                 verbose=1)\n    \n    # --------------------------\n    # Evaluate Best Model\n    # --------------------------\n    best_model = tuner.get_best_models(num_models=1)[0]\n    loss, acc = best_model.evaluate(val_dataset)\n    print(f\"\\nTest Accuracy: {acc*100:.2f}%\")\n    \n    best_model.save(\"/kaggle/working/deep_learning_model.h5\")\n\n\n# --------------------------\n# Predictions & Confusion Matrix\n# --------------------------\nloss, acc = best_model.evaluate(val_dataset)\nprint(f\"\\nTest Accuracy: {acc*100:.2f}%\")\ny_pred_probs = best_model.predict(val_dataset)\ny_pred = np.argmax(y_pred_probs, axis=1)\n\ny_true_labels = label_encoder.inverse_transform(y_test)\ny_pred_labels = label_encoder.inverse_transform(y_pred)\n\ncm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_encoder.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\ndisp.plot(cmap='Blues', xticks_rotation=45)\nplt.title(\"Test Set Confusion Matrix\")\nplt.show()\n\n# --------------------------\n# F1 Scores & Classification Report\n# --------------------------\nmacro_f1 = f1_score(y_test, y_pred, average='macro')\nmicro_f1 = f1_score(y_test, y_pred, average='micro')\nweighted_f1 = f1_score(y_test, y_pred, average='weighted')\n\nprint(\"\\nF1 Scores:\")\nprint(f\"Macro-F1:    {macro_f1:.4f}\")\nprint(f\"Micro-F1:    {micro_f1:.4f}\")\nprint(f\"Weighted-F1: {weighted_f1:.4f}\")\n\nprint(\"\\nDetailed classification report:\")\nprint(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T17:26:25.818667Z","iopub.execute_input":"2025-11-06T17:26:25.819100Z","iopub.status.idle":"2025-11-06T17:26:31.115854Z","shell.execute_reply.started":"2025-11-06T17:26:25.819071Z","shell.execute_reply":"2025-11-06T17:26:31.114592Z"}},"outputs":[],"execution_count":null}]}
